{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf40bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "#import imageio, skimage\n",
    "\n",
    "import torch\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e71eee",
   "metadata": {},
   "source": [
    "## Example from VirTex\n",
    "\n",
    "code from `arch-pre-training/virtex/data/datasets/coco_captions.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e4c4fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from virtex/data/datasets/coco_captions.py\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import cv2\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b088b7",
   "metadata": {},
   "source": [
    "## Unified Dataset Class for ARCH Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44907528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchCaptionsDatasetRaw(Dataset):\n",
    "    r\"\"\"\n",
    "    A PyTorch dataset to read ARCH dataset and provide it completely\n",
    "    unprocessed. This dataset is used by various task-specific datasets\n",
    "    in :mod:`~virtex.data.datasets` module.\n",
    "\n",
    "    Args:\n",
    "        data_root: Path to the ARCH dataset root directory.\n",
    "        source: Name of ARCH source to read. One of ``{\"pubmed\", \"books\", \"both\"}``. Default value: \"both\".\n",
    "        split:  Name of ARCH split to read. One of ``{\"train\", \"val\", \"all\"}``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_root: str, source: str='both', split: str=''):\n",
    "        allowed_source_values = ['pubmed', 'books', 'both']\n",
    "        assert source in allowed_source_values, f\"source should be one of {allowed_source_values}\"\n",
    "        allowed_split_values = ['train', 'val', 'all']\n",
    "        assert split in allowed_split_values, f\"split should be one of {allowed_split_values}\"\n",
    "\n",
    "        # Get path to the annotation file\n",
    "        captions = json.load(\n",
    "            open(os.path.join(data_root, \"annotations\", f\"captions_{split}.json\"))\n",
    "        )\n",
    "        \n",
    "        # Collect list of uuids and file paths for each caption\n",
    "        captions_to_uuids: Dict[str, List[str]] = defaultdict(list)\n",
    "        captions_to_image_filepaths: Dict[str, List[str]] = defaultdict(list)\n",
    "        for idx, ann in captions.items():\n",
    "            if (source == \"both\") or (source == ann['source']):\n",
    "                # if source=\"both\", then no filtering needed\n",
    "                # if source is one of the [\"books\", \"pubmed\"], LHS=False, RHS will filter the needed captions\n",
    "\n",
    "                # make a check that the image exist before adding its `uuid` or `path`\n",
    "                assert os.path.exists(ann['path']), f\"{ann['path']} does not exist!\"\n",
    "                \n",
    "                captions_to_uuids[ann['caption']].append(ann['uuid'])\n",
    "                captions_to_image_filepaths[ann['caption']].append(ann['path'])\n",
    "        #print(captions_per_image)\n",
    "\n",
    "        # Keep all annotations in memory. Make a list of tuples, each tuple\n",
    "        # is ``(list[image_id], list[file_path], captions)``.\n",
    "        self.instances = [\n",
    "            (captions_to_uuids[caption], captions_to_image_filepaths[caption], caption)\n",
    "            for caption in captions_to_image_filepaths.keys()\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_ids, image_paths, caption = self.instances[idx]\n",
    "\n",
    "        # shape: (height, width, channels), dtype: uint8\n",
    "        images = [cv2.imread(image_path) for image_path in image_paths]\n",
    "        # cv2.imread loads images in BGR (blue, green, red) order\n",
    "        images = [cv2.cvtColor(image, cv2.COLOR_BGR2RGB) for image in images]\n",
    "\n",
    "        return {\"image_ids\": image_ids, \"images\": images, \"caption\": caption}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a85f02ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3210"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arch_books_dataset = ArchCaptionsDatasetRaw(data_root='../datasets/ARCH',\n",
    "                                            source=\"books\",\n",
    "                                            split='all')\n",
    "len(arch_books_dataset.instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "325120ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3285"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arch_pubmed_dataset = ArchCaptionsDatasetRaw(data_root='../datasets/ARCH',\n",
    "                                             source=\"pubmed\",\n",
    "                                             split=\"all\")\n",
    "len(arch_pubmed_dataset.instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49455ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5196"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arch_dataset_raw = ArchCaptionsDatasetRaw(data_root='../datasets/ARCH',\n",
    "                                          source=\"both\",\n",
    "                                          split=\"train\")\n",
    "len(arch_dataset_raw.instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55874161",
   "metadata": {},
   "source": [
    "## Unified Dataset Class + augmentations and collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f537d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Callable, Dict, List\n",
    "\n",
    "import albumentations as alb\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from virtex.data.tokenizers import SentencePieceBPETokenizer\n",
    "from virtex.data import transforms as T\n",
    "from virtex.data.datasets.coco_captions import CocoCaptionsDataset\n",
    "\n",
    "\n",
    "class ArchCaptioningDatasetExtended(Dataset):\n",
    "    r\"\"\"\n",
    "    A dataset which provides image-caption (forward and backward) pairs from\n",
    "    a ARCH Captions annotation file. This is used for pretraining tasks which\n",
    "    use captions - bicaptioning, forward captioning and token classification.\n",
    "\n",
    "    Args:\n",
    "        data_root: Path to dataset directory containing images and annotations.\n",
    "        source: Name of ARCH source to read. One of ``{\"pubmed\", \"books\", \"both\"}``.\n",
    "            \"both\" option results in a concatenation of the datasets from \"pubmed\" and \"books\"\n",
    "        split: Name of ARCH split to read. One of ``{\"train\", \"val\", \"all\"}``.\n",
    "        tokenizer: Tokenizer which maps word tokens to their integer IDs.\n",
    "        image_transform: List of image transformations, from either\n",
    "            `albumentations <https://albumentations.readthedocs.io/en/latest/>`_\n",
    "            or :mod:`virtex.data.transforms`.\n",
    "        max_caption_length: Maximum number of tokens to keep in caption tokens.\n",
    "            Extra tokens will be trimmed from the right end of the token list.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        source: str,\n",
    "        split: str,\n",
    "        tokenizer: SentencePieceBPETokenizer,\n",
    "        image_transform: Callable = T.DEFAULT_IMAGE_TRANSFORM,\n",
    "        max_caption_length: int = 30,\n",
    "    ):\n",
    "        self._dset = ArchCaptionsDatasetRaw(data_root=data_root, source=source, split=split)\n",
    "        self.image_transform = image_transform\n",
    "        self.caption_transform = alb.Compose(\n",
    "            [\n",
    "                T.NormalizeCaption(),\n",
    "                T.TokenizeCaption(tokenizer),\n",
    "                T.TruncateCaptionTokens(max_caption_length),\n",
    "            ]\n",
    "        )\n",
    "        self.padding_idx = tokenizer.token_to_id(\"<unk>\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        # keys: {\"image_id\", \"image\", \"captions\"}\n",
    "        instance = self._dset[idx]\n",
    "        image_ids, images, caption = (\n",
    "            instance[\"image_ids\"],\n",
    "            instance[\"images\"],\n",
    "            instance[\"caption\"],\n",
    "        )\n",
    "        \n",
    "        # list of np.arrays -> torch tensor\n",
    "        # transformation needs to be done before putting into tensor\n",
    "        # (need the same size for each image to put into tensor)\n",
    "        # \n",
    "        # TODO: think how to do apply image_transform to each image before putting into tensor\n",
    "        images = torch.stack([torch.from_numpy(image) for image in images], dim=0)\n",
    "        \n",
    "        print(image_ids)\n",
    "        print(images)\n",
    "        print(caption)\n",
    "\n",
    "        # Transform image-caption pair and convert image from HWC to CHW format.\n",
    "        # Pass in caption to image_transform due to paired horizontal flip.\n",
    "        # Caption won't be tokenized/processed here.\n",
    "        images_caption = self.image_transform(image=images, caption=caption)\n",
    "        images, caption = images_caption[\"images\"], image_caption[\"caption\"]\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "        caption_tokens = self.caption_transform(caption=caption)[\"caption\"]\n",
    "        return {\n",
    "            \"images_ids\": torch.tensor(image_ids, dtype=torch.str),\n",
    "            \"images\": torch.tensor(image, dtype=torch.float),\n",
    "            \"caption_tokens\": torch.tensor(caption_tokens, dtype=torch.long),\n",
    "            \"noitpac_tokens\": torch.tensor(caption_tokens, dtype=torch.long).flip(0),\n",
    "            \"caption_lengths\": torch.tensor(len(caption_tokens), dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def collate_fn(\n",
    "        self, data: List[Dict[str, torch.Tensor]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        # Pad `caption_tokens` and `masked_labels` up to this length.\n",
    "        caption_tokens = torch.nn.utils.rnn.pad_sequence(\n",
    "            [d[\"caption_tokens\"] for d in data],\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_idx,\n",
    "        )\n",
    "        noitpac_tokens = torch.nn.utils.rnn.pad_sequence(\n",
    "            [d[\"noitpac_tokens\"] for d in data],\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_idx,\n",
    "        )\n",
    "        return {\n",
    "            \"image_id\": torch.stack([d[\"image_ids\"] for d in data], dim=0),\n",
    "            \"image\": torch.stack([d[\"images\"] for d in data], dim=0),\n",
    "            \"caption_tokens\": caption_tokens,\n",
    "            \"noitpac_tokens\": noitpac_tokens,\n",
    "            \"caption_lengths\": torch.stack([d[\"caption_lengths\"] for d in data]),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c261477e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5196"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arch_tokenizer = SentencePieceBPETokenizer(\"../datasets/vocab/arch_10k.model\")\n",
    "\n",
    "arch_dataset_extended = ArchCaptioningDatasetExtended(data_root='../datasets/ARCH',\n",
    "                                                      source=\"both\", split=\"train\",\n",
    "                                                      tokenizer=arch_tokenizer)\n",
    "len(arch_dataset_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e313b6c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [671, 907, 3] at entry 0 and [674, 910, 3] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lz/602r_tbj50jf1yw4jnmxbg9r0000gn/T/ipykernel_12317/2910216234.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0march_dataset_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/lz/602r_tbj50jf1yw4jnmxbg9r0000gn/T/ipykernel_12317/2943532106.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# TODO: think how to do apply image_transform to each image before putting into tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [671, 907, 3] at entry 0 and [674, 910, 3] at entry 1"
     ]
    }
   ],
   "source": [
    "arch_dataset_extended.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d2eaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:virtex] *",
   "language": "python",
   "name": "conda-env-virtex-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
