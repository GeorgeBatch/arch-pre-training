{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf40bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "#import imageio, skimage\n",
    "\n",
    "import torch\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pwd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "print(\"Device count :\", torch.cuda.device_count())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "id": "de3a7c22",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%reload_ext autoreload"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9256fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from virtex.data.datasets.arch_captions import ArchCaptionsDatasetRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b088b7",
   "metadata": {},
   "source": [
    "## Unified Dataset Class for ARCH Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_dataset_raw_train = ArchCaptionsDatasetRaw(data_root='../datasets/ARCH',\n",
    "                                                source=\"both\",\n",
    "                                                split=\"train\")\n",
    "len(arch_dataset_raw_train.instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55874161",
   "metadata": {},
   "source": [
    "## Unified Dataset Class + augmentations and collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f537d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Callable, Dict, List\n",
    "\n",
    "import albumentations as alb\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from virtex.data.tokenizers import SentencePieceBPETokenizer\n",
    "from virtex.data import transforms as T\n",
    "from virtex.data.datasets.arch_captions import ArchCaptionsDatasetRaw\n",
    "\n",
    "\n",
    "class ArchCaptioningDatasetExtended(Dataset):\n",
    "    r\"\"\"\n",
    "    A dataset which provides image-caption (forward and backward) pairs from\n",
    "    a ARCH Captions annotation file. This is used for pretraining tasks which\n",
    "    use captions - bicaptioning, forward captioning and token classification.\n",
    "\n",
    "    Args:\n",
    "        data_root: Path to dataset directory containing images and annotations.\n",
    "        source: Name of ARCH source to read. One of ``{\"pubmed\", \"books\", \"both\"}``.\n",
    "            \"both\" option results in a concatenation of the datasets from \"pubmed\" and \"books\"\n",
    "        split: Name of ARCH split to read. One of ``{\"train\", \"val\", \"all\"}``.\n",
    "        tokenizer: Tokenizer which maps word tokens to their integer IDs.\n",
    "        image_transform: List of image transformations, from either\n",
    "            `albumentations <https://albumentations.readthedocs.io/en/latest/>`_\n",
    "            or :mod:`virtex.data.transforms`.\n",
    "        max_caption_length: Maximum number of tokens to keep in caption tokens.\n",
    "            Extra tokens will be trimmed from the right end of the token list.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_root: str,\n",
    "            split: str,\n",
    "            tokenizer: SentencePieceBPETokenizer,\n",
    "            source: str = \"both\",\n",
    "            image_transform: Callable = T.ARCH_DEFAULT_IMAGE_TRANSFORM,\n",
    "            tensor_flip_transform: Callable = None,\n",
    "            max_caption_length: int = 30,\n",
    "    ):\n",
    "        self._dset = ArchCaptionsDatasetRaw(data_root=data_root, source=source,\n",
    "                                            split=split)\n",
    "        self.image_transform = image_transform\n",
    "        self.tensor_flip_transform = tensor_flip_transform\n",
    "        self.caption_transform = alb.Compose(\n",
    "            [\n",
    "                T.NormalizeCaption(),\n",
    "                T.TokenizeCaption(tokenizer),\n",
    "                T.TruncateCaptionTokens(max_caption_length),\n",
    "            ]\n",
    "        )\n",
    "        self.padding_idx = tokenizer.token_to_id(\"<unk>\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        # keys: {\"image_ids\", \"images\", \"caption\"}\n",
    "        instance = self._dset[idx]\n",
    "        image_ids, images, caption = (\n",
    "            instance[\"image_ids\"],\n",
    "            instance[\"images\"],\n",
    "            instance[\"caption\"],\n",
    "        )\n",
    "\n",
    "        # # debugging\n",
    "        # print(\"Checkpoint 1\")\n",
    "        # print(\"Shapes before applying self.image_transform\", [image.shape for image in images])\n",
    "\n",
    "        # List[int] -> np.array of shape (len(image_ids), )\n",
    "        image_ids = np.array(image_ids)\n",
    "        # (len(image_ids), ) -> (len(image_ids), 1)\n",
    "        image_ids = image_ids.reshape((image_ids.shape[0], 1))\n",
    "\n",
    "        # # debugging\n",
    "        # print(\"Checkpoint 2\")\n",
    "\n",
    "        # Transform images, no flips at this stage not to create multiple versions of the caption!\n",
    "        #     Before flipping all images need to be resized to the same size to put them into a tensor.\n",
    "        #     Caption won't be tokenized/processed here.\n",
    "        #     Albumentations transforms require named arguments - can't avoid it.\n",
    "\n",
    "        images = [self.image_transform(image=image)[\"image\"] for image in\n",
    "                  images]\n",
    "        # print(\"Shapes after applying self.image_transform\", [image.shape for image in images])\n",
    "\n",
    "        # # # debugging\n",
    "        # print(\"Checkpoint 3\")\n",
    "\n",
    "        # Convert each image from HWC to CHW format and convert to tensors:\n",
    "        #     PyTorch Transforms expect to receive tensors in (B, C, H, W) shape\n",
    "        #     [(Channel, Height, Width), ..., ] Bag Size times\n",
    "        images = [np.transpose(image, (2, 0, 1)) for image in images]\n",
    "        images = [torch.tensor(image, dtype=torch.float) for image in images]\n",
    "\n",
    "        # # # debugging\n",
    "        # print(\"Checkpoint 4\")\n",
    "\n",
    "        # stack all the images into a tensor: (bag_size=batch_size, Channel, Height, Width)\n",
    "        images = torch.stack(images, dim=0)\n",
    "\n",
    "        if self.tensor_flip_transform is not None:\n",
    "            # perform tensor transforms on images in the tensor and the\n",
    "            # corresponding caption, e.g. random horizontal flips\n",
    "            # Reason: single version of the caption should appear => random flip\n",
    "            # should be performed on all images in a bag\n",
    "            images_caption = self.tensor_flip_transform(image=images, caption=caption)\n",
    "            images, caption = images_caption[\"image\"], images_caption[\"caption\"]\n",
    "\n",
    "        # print(images)\n",
    "        # print(caption)\n",
    "\n",
    "        # # # debugging\n",
    "        # print(\"Checkpoint 5\")\n",
    "\n",
    "        # caption tokens\n",
    "        caption_tokens = self.caption_transform(caption=caption)[\"caption\"]\n",
    "\n",
    "        # # # debugging\n",
    "        # print(\"Checkpoint 6\")\n",
    "\n",
    "        return {\n",
    "            \"image_ids\": torch.tensor(image_ids, dtype=torch.long), #(bag_size,1)\n",
    "            \"images\": images,\n",
    "            \"caption_tokens\": torch.tensor(caption_tokens, dtype=torch.long),\n",
    "            \"noitpac_tokens\": torch.tensor(caption_tokens,\n",
    "                                           dtype=torch.long).flip(0),\n",
    "            \"caption_lengths\": torch.tensor(len(caption_tokens),\n",
    "                                            dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def collate_fn(\n",
    "            self, data: List[Dict[str, torch.Tensor]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # Pad `caption_tokens` and `masked_labels` up to this length.\n",
    "        caption_tokens = torch.nn.utils.rnn.pad_sequence(\n",
    "            [d[\"caption_tokens\"] for d in data],\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_idx,\n",
    "        )\n",
    "        noitpac_tokens = torch.nn.utils.rnn.pad_sequence(\n",
    "            [d[\"noitpac_tokens\"] for d in data],\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_idx,\n",
    "        )\n",
    "        return {\n",
    "            \"image_id\": torch.stack([d[\"image_ids\"] for d in data], dim=0),\n",
    "            \"image\": torch.stack([d[\"images\"] for d in data], dim=0),\n",
    "            \"caption_tokens\": caption_tokens,\n",
    "            \"noitpac_tokens\": noitpac_tokens,\n",
    "            \"caption_lengths\": torch.stack(\n",
    "                [d[\"caption_lengths\"] for d in data]),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check the default transform\n",
    "T.DEFAULT_IMAGE_TRANSFORM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "T.ARCH_DEFAULT_IMAGE_TRANSFORM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "help(T.DEFAULT_FLIP_TRANSFORM)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2da09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(SentencePieceBPETokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c261477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_tokenizer = SentencePieceBPETokenizer(\"../datasets/vocab/arch_10k.model\")\n",
    "\n",
    "arch_train_dataset_extended = ArchCaptioningDatasetExtended(data_root='../datasets/ARCH',\n",
    "                                                      split=\"train\",\n",
    "                                                      tokenizer=arch_tokenizer,\n",
    "                                                      tensor_flip_transform=None)\n",
    "len(arch_train_dataset_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e313b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_item = arch_train_dataset_extended.__getitem__(0)\n",
    "sample_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_item['images'].size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c203b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tensor_image = sample_item['images'][0]\n",
    "sample_tensor_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(sample_tensor_image.to(int).view(sample_tensor_image.shape[1],\n",
    "                                     sample_tensor_image.shape[2], sample_tensor_image.shape[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test how the dataloader works\n",
    "\n",
    "This will help to debug the `collate_fn`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        dataset=arch_train_dataset_extended,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=arch_train_dataset_extended.collate_fn,\n",
    "    )\n",
    "\n",
    "len(train_dataloader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(\"image_id\", batch[\"image_id\"])\n",
    "    print(\"image shape\", batch[\"image\"].shape)\n",
    "    print(\"caption_tokens:\", batch[\"caption_tokens\"])\n",
    "    print(\"noitpac_tokens:\", batch[\"noitpac_tokens\"])\n",
    "    print(\"caption_lengths:\", batch[\"caption_lengths\"])\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch['image'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "090abd03",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Examples of TensorHorizontalFlip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flip = T.TensorHorizontalFlip(p=0.5)\n",
    "img_tensor = torch.Tensor([[[1, 1, 0, 0] for _ in range(4)],\n",
    "                           [[0, 0, 1, 1] for _ in range(4)]])\n",
    "print(img_tensor.shape)\n",
    "img_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_caption = flip(image=img_tensor, caption=\"1-s are to the left on the \"\n",
    "                                               \"first image and to the right \"\n",
    "                                               \"on the second\")\n",
    "print(image_caption['image'])\n",
    "print(image_caption['caption'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking if the horizontal flip can be applied to different images separately in torchvision"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from PIL import Image\n",
    "test_transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    torchvision.transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "fixed_test_transform = test_transform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_list = [np.array([1, 1, 0, 0], dtype=np.float32).reshape((1, 4)) for _ in range(20)]\n",
    "test_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[fixed_test_transform(l) for l in test_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It can not be applied separately, so the flip needs to be applied on the tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:virtex] *",
   "language": "python",
   "name": "conda-env-virtex-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}