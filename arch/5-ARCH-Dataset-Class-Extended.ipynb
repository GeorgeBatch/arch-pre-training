{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf40bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "#import imageio, skimage\n",
    "\n",
    "import torch\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a7c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9256fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from virtex.data.datasets.arch_captions import ArchCaptionsDatasetRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b088b7",
   "metadata": {},
   "source": [
    "## Unified Dataset Class for ARCH Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_dataset_raw_train = ArchCaptionsDatasetRaw(data_root='../datasets/ARCH',\n",
    "                                                source=\"both\",\n",
    "                                                split=\"train\")\n",
    "len(arch_dataset_raw_train.instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55874161",
   "metadata": {},
   "source": [
    "## Unified Dataset Class + augmentations and collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a45a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the default transform\n",
    "T.DEFAULT_IMAGE_TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81d2fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f537d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Callable, Dict, List\n",
    "\n",
    "import albumentations as alb\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from virtex.data.tokenizers import SentencePieceBPETokenizer\n",
    "from virtex.data import transforms as T\n",
    "from virtex.data.datasets.arch_captions import ArchCaptionsDatasetRaw\n",
    "\n",
    "\n",
    "class ArchCaptioningDatasetExtended(Dataset):\n",
    "    r\"\"\"\n",
    "    A dataset which provides image-caption (forward and backward) pairs from\n",
    "    a ARCH Captions annotation file. This is used for pretraining tasks which\n",
    "    use captions - bicaptioning, forward captioning and token classification.\n",
    "\n",
    "    Args:\n",
    "        data_root: Path to dataset directory containing images and annotations.\n",
    "        source: Name of ARCH source to read. One of ``{\"pubmed\", \"books\", \"both\"}``.\n",
    "            \"both\" option results in a concatenation of the datasets from \"pubmed\" and \"books\"\n",
    "        split: Name of ARCH split to read. One of ``{\"train\", \"val\", \"all\"}``.\n",
    "        tokenizer: Tokenizer which maps word tokens to their integer IDs.\n",
    "        image_transform: List of image transformations, from either\n",
    "            `albumentations <https://albumentations.readthedocs.io/en/latest/>`_\n",
    "            or :mod:`virtex.data.transforms`.\n",
    "        max_caption_length: Maximum number of tokens to keep in caption tokens.\n",
    "            Extra tokens will be trimmed from the right end of the token list.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        split: str,\n",
    "        tokenizer: SentencePieceBPETokenizer,\n",
    "        source: str=\"both\",\n",
    "        image_transform: Callable = T.DEFAULT_IMAGE_TRANSFORM,\n",
    "        max_caption_length: int = 30,\n",
    "    ):\n",
    "        self._dset = ArchCaptionsDatasetRaw(data_root=data_root, source=source, split=split)\n",
    "        self.image_transform = image_transform\n",
    "        self.caption_transform = alb.Compose(\n",
    "            [\n",
    "                T.NormalizeCaption(),\n",
    "                T.TokenizeCaption(tokenizer),\n",
    "                T.TruncateCaptionTokens(max_caption_length),\n",
    "            ]\n",
    "        )\n",
    "        self.padding_idx = tokenizer.token_to_id(\"<unk>\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        # keys: {\"image_ids\", \"images\", \"caption\"}\n",
    "        instance = self._dset[idx]\n",
    "        image_ids, images, caption = (\n",
    "            instance[\"image_ids\"],\n",
    "            instance[\"images\"],\n",
    "            instance[\"caption\"],\n",
    "        )\n",
    "        \n",
    "        # debugging\n",
    "        print(\"Checkpoint 1\")\n",
    "        \n",
    "        # List[int] -> np.array of shape (len(image_ids), )\n",
    "        image_ids = np.array(image_ids)\n",
    "        # (len(image_ids), ) -> (len(image_ids), 1)\n",
    "        image_ids = image_ids.reshape((image_ids.shape[0], 1))\n",
    "        \n",
    "        \n",
    "        # debugging\n",
    "        print(\"Checkpoint 2\")\n",
    "        \n",
    "#         # Transform images, no flips at this stage not to create multiple versions of the caption!\n",
    "#         #     Before flipping all images need to be resized to the same size to put them into a tensor.\n",
    "#         #     Caption won't be tokenized/processed here.\n",
    "#         #     Albumentation transforms require named argumants - can't aviod it.\n",
    "#         image_caption_pairs = [self.image_transform(image=image, caption=caption) for image in images]\n",
    "        \n",
    "        \n",
    "#         # debugging\n",
    "#         print(\"Checkpoint 3\")\n",
    "        \n",
    "#         # split images and captions (all captions should be the same)\n",
    "#         images = [image_caption[\"image\"] for image_caption in image_caption_pairs]\n",
    "#         captions = [image_caption[\"caption\"] for image_caption in image_caption_pairs]\n",
    "        \n",
    "#         # debugging\n",
    "#         print(\"Checkpoint 4\")\n",
    "        \n",
    "#         assert all([captions[0] == captions[i] for i in range(len(captions))]), \\\n",
    "#             \"Make sure no random flips are performed in the self.image_transform. \\\n",
    "#             A random flip will be performed later on the whole bag of images.\"\n",
    "\n",
    "        images = [self.image_transform(image=image)[\"image\"] for image in images]\n",
    "        \n",
    "        # debugging\n",
    "        print(\"Checkpoint 5\")\n",
    "        \n",
    "        # Convert each image from HWC to CHW format and convert to tensors:\n",
    "        #     Transforms expect to receive tensors in (B, C, H, W) shape\n",
    "        #     [(Channel, Height, Width), ..., ] Bag Size times\n",
    "        images = [np.transpose(image, (2, 0, 1)) for image in images]\n",
    "        images = [torch.tensor(image, dtype=torch.float) for image in images]\n",
    "    \n",
    "    \n",
    "        # debugging\n",
    "        print(\"Checkpoint 6\")\n",
    "        \n",
    "        # stack all the images into a tensor: (bag_size=batch_size, Channel, Height, Width)\n",
    "        images = torch.stack(images, dim=0)\n",
    "        \n",
    "        # debugging\n",
    "        print(\"Checkpoint 7\")\n",
    "        \n",
    "        # single version of the caption should appear => random flip should be performed on all images in a bag\n",
    "        # TODO: perform random flips on the tensor of images and the caption together\n",
    "        \n",
    "        # caption tokens\n",
    "        caption_tokens = self.caption_transform(caption=caption)[\"caption\"]\n",
    "        \n",
    "        \n",
    "        # debugging\n",
    "        print(\"Checkpoint 9\")\n",
    "\n",
    "        \n",
    "        return {\n",
    "            \"image_ids\": torch.tensor(image_ids, dtype=torch.long), # ((Bag size, 1)\n",
    "            \"images\": images,\n",
    "            \"caption_tokens\": torch.tensor(caption_tokens, dtype=torch.long),\n",
    "            \"noitpac_tokens\": torch.tensor(caption_tokens, dtype=torch.long).flip(0),\n",
    "            \"caption_lengths\": torch.tensor(len(caption_tokens), dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def collate_fn(\n",
    "        self, data: List[Dict[str, torch.Tensor]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        # Pad `caption_tokens` and `masked_labels` up to this length.\n",
    "        caption_tokens = torch.nn.utils.rnn.pad_sequence(\n",
    "            [d[\"caption_tokens\"] for d in data],\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_idx,\n",
    "        )\n",
    "        noitpac_tokens = torch.nn.utils.rnn.pad_sequence(\n",
    "            [d[\"noitpac_tokens\"] for d in data],\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_idx,\n",
    "        )\n",
    "        return {\n",
    "            \"image_id\": torch.stack([d[\"image_ids\"] for d in data], dim=0),\n",
    "            \"image\": torch.stack([d[\"images\"] for d in data], dim=0),\n",
    "            \"caption_tokens\": caption_tokens,\n",
    "            \"noitpac_tokens\": noitpac_tokens,\n",
    "            \"caption_lengths\": torch.stack([d[\"caption_lengths\"] for d in data]),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2da09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(SentencePieceBPETokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c261477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_tokenizer = SentencePieceBPETokenizer(\"../datasets/vocab/arch_10k.model\")\n",
    "\n",
    "arch_dataset_extended = ArchCaptioningDatasetExtended(data_root='../datasets/ARCH',\n",
    "                                                      #source=\"both\",\n",
    "                                                      split=\"train\",\n",
    "                                                      tokenizer=arch_tokenizer)\n",
    "len(arch_dataset_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e313b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_dataset_extended.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c203b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from PIL import Image\n",
    "test_transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    torchvision.transforms.ToTensor()]\n",
    ")    \n",
    "\n",
    "fixed_test_transform = test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c00045",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [np.array([1, 1, 0, 0], dtype=np.float32).reshape((1, 4)) for _ in range(100)]\n",
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa63e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "[fixed_test_transform(l) for l in test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3829c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090abd03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:virtex] *",
   "language": "python",
   "name": "conda-env-virtex-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
