{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf40bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "#import imageio, skimage\n",
    "\n",
    "import torch\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a8c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS_DIR = '../datasets/ARCH/annotations'\n",
    "os.listdir(ANNOTATIONS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e71eee",
   "metadata": {},
   "source": [
    "## Example from VirTex\n",
    "\n",
    "code from `arch-pre-training/virtex/data/datasets/coco_captions.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c4fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from virtex/data/datasets/coco_captions.py\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import cv2\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375694c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUSTOM = True\n",
    "\n",
    "if not USE_CUSTOM:\n",
    "    from virtex.data.datasets.coco_captions import CocoCaptionsDataset\n",
    "else:\n",
    "    class CocoCaptionsDataset(Dataset):\n",
    "        r\"\"\"\n",
    "        A PyTorch dataset to read COCO Captions dataset and provide it completely\n",
    "        unprocessed. This dataset is used by various task-specific datasets\n",
    "        in :mod:`~virtex.data.datasets` module.\n",
    "\n",
    "        Args:\n",
    "            data_root: Path to the COCO dataset root directory.\n",
    "            split: Name of COCO 2017 split to read. One of ``{\"train\", \"val\"}``.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, data_root: str, split: str):\n",
    "\n",
    "            # Get paths to image directory and annotation file.\n",
    "            image_dir = os.path.join(data_root, f\"{split}2017\")\n",
    "            captions = json.load(\n",
    "                open(os.path.join(data_root, \"annotations\", f\"captions_{split}2017.json\"))\n",
    "            )\n",
    "            # Collect list of captions for each image.\n",
    "            captions_per_image: Dict[int, List[str]] = defaultdict(list)\n",
    "            for ann in captions[\"annotations\"]:\n",
    "                captions_per_image[ann[\"image_id\"]].append(ann[\"caption\"])\n",
    "\n",
    "            # Collect image file for each image (by its ID).\n",
    "            image_filepaths: Dict[int, str] = {\n",
    "                im[\"id\"]: os.path.join(image_dir, im[\"file_name\"])\n",
    "                for im in captions[\"images\"]\n",
    "            }\n",
    "            # Keep all annotations in memory. Make a list of tuples, each tuple\n",
    "            # is ``(image_id, file_path, list[captions])``.\n",
    "            self.instances = [\n",
    "                (im_id, image_filepaths[im_id], captions_per_image[im_id])\n",
    "                for im_id in captions_per_image.keys()\n",
    "            ]\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.instances)\n",
    "\n",
    "        def __getitem__(self, idx: int):\n",
    "            image_id, image_path, captions = self.instances[idx]\n",
    "\n",
    "            # shape: (height, width, channels), dtype: uint8\n",
    "            try:\n",
    "                image = cv2.imread(image_path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "            return {\"image_id\": image_id, \"image\": image, \"captions\": captions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1101019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from virtex.data.datasets.arch_captions import ArchCaptionsDatasetRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a2d20",
   "metadata": {},
   "source": [
    "Check how the dataset object behaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataset = CocoCaptionsDataset('../datasets/coco', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d8edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataset.instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee16de",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9eb8c0",
   "metadata": {},
   "source": [
    "## PubMed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_set_dir = '../datasets/ARCH/pubmed_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b45be",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(pubmed_set_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717728ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(pubmed_set_dir +'/images')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b36a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Old version\n",
    "\n",
    "# class ArchPubmedCaptionsDataset(Dataset):\n",
    "#     r\"\"\"\n",
    "#     A PyTorch dataset to read ARCH Pubmed dataset and provide it completely\n",
    "#     unprocessed. This dataset is used by various task-specific datasets\n",
    "#     in :mod:`~virtex.data.datasets` module.\n",
    "\n",
    "#     Args:\n",
    "#         data_root: Path to the ARCH dataset root directory.\n",
    "#         split: Name of ARCH split to read. One of ``{\"train\", \"val\"}``.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, data_root: str, split: str=''):\n",
    "\n",
    "# #         TODO: change after splitting the caption files into train and validation\n",
    "# #         \n",
    "# #         # Get paths to image directory and annotation file.\n",
    "# #         image_dir = os.path.join(data_root, \"pubmed_set/images\", f\"{split}\")\n",
    "# #         captions = json.load(\n",
    "# #             open(os.path.join(data_root, \"pubmed_set\", f\"captions_{split}.json\"))\n",
    "# #         )\n",
    "        \n",
    "#         # Get paths to image directory and annotation file.\n",
    "#         image_dir = os.path.join(data_root, \"pubmed_set/images\")\n",
    "#         captions = json.load(\n",
    "#             open(os.path.join(data_root, \"pubmed_set\", \"captions.json\"))\n",
    "#         )\n",
    "        \n",
    "#         # Collect list of captions for each image.\n",
    "#         captions_per_image: Dict[int, List[str]] = defaultdict(list)\n",
    "#         for idx, ann in captions.items():\n",
    "#             captions_per_image[ann['uuid']].append(ann['caption'])\n",
    "#         #print(captions_per_image)\n",
    "\n",
    "#         # Collect image file for each image (by its ID).\n",
    "#         image_filepaths: Dict[int, str] = {\n",
    "#             ann[\"uuid\"]: os.path.join(image_dir, f\"{ann['uuid']}.jpg\")\n",
    "#             for idx, ann in captions.items()\n",
    "#         }\n",
    "#         # Keep all annotations in memory. Make a list of tuples, each tuple\n",
    "#         # is ``(image_id, file_path, list[captions])``.\n",
    "#         self.instances = [\n",
    "#             (im_id, image_filepaths[im_id], captions_per_image[im_id])\n",
    "#             for im_id in captions_per_image.keys()\n",
    "#         ]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.instances)\n",
    "\n",
    "#     def __getitem__(self, idx: int):\n",
    "#         image_id, image_path, captions = self.instances[idx]\n",
    "\n",
    "#         # shape: (height, width, channels), dtype: uint8\n",
    "#         image = cv2.imread(image_path)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # cv2.imread loads images in BGR (blue, green, red) order\n",
    "\n",
    "#         return {\"image_id\": image_id, \"image\": image, \"captions\": captions}\n",
    "    \n",
    "    \n",
    "# # test\n",
    "# arch_pubmed_dataset = ArchPubmedCaptionsDataset('../datasets/ARCH')\n",
    "# arch_pubmed_dataset.instances\n",
    "\n",
    "# test_instance = arch_pubmed_dataset.__getitem__(0)\n",
    "\n",
    "# print(test_instance['image_id'])\n",
    "# print(test_instance['image'].shape)\n",
    "\n",
    "\n",
    "# plt.imshow(test_instance['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7de5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchPubmedCaptionsDataset(Dataset):\n",
    "    r\"\"\"\n",
    "    A PyTorch dataset to read ARCH Pubmed dataset and provide it completely\n",
    "    unprocessed. This dataset is used by various task-specific datasets\n",
    "    in :mod:`~virtex.data.datasets` module.\n",
    "\n",
    "    Args:\n",
    "        data_root: Path to the ARCH dataset root directory.\n",
    "        split: Name of ARCH split to read. One of ``{\"train\", \"val\", \"all\"}``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_root: str, split: str=''):\n",
    "\n",
    "        # Get path to image directory and record the extensions\n",
    "        image_dir = os.path.join(data_root, \"pubmed_set/images\")\n",
    "        uuids_to_extensions = {\n",
    "            file_name.split('.')[0]: file_name.split('.')[1]\n",
    "            for file_name in os.listdir(image_dir)\n",
    "        }\n",
    "        \n",
    "        # Get path to the annotation file\n",
    "        captions = json.load(\n",
    "            open(os.path.join(data_root, \"annotations\", f\"captions_{split}.json\"))\n",
    "        )\n",
    "        \n",
    "        # Collect list of uuids and file paths for each caption\n",
    "        captions_to_uuids: Dict[str, List[str]] = defaultdict(list)\n",
    "        captions_to_image_filepaths: Dict[str, List[str]] = defaultdict(list)\n",
    "        for idx, ann in captions.items():\n",
    "            if ann['uuid'] in uuids_to_extensions.keys():\n",
    "                # uuids_to_extensions contains only image uuids from the image dir\n",
    "                # this means that only uuids with exisitng images will be added\n",
    "                captions_to_uuids[ann['caption']].append(ann['uuid'])\n",
    "                captions_to_image_filepaths[ann['caption']].append(ann['path'])\n",
    "        #print(captions_per_image)\n",
    "\n",
    "        # Keep all annotations in memory. Make a list of tuples, each tuple\n",
    "        # is ``(list[image_id], list[file_path], captions)``.\n",
    "        self.instances = [\n",
    "            (captions_to_uuids[caption], captions_to_image_filepaths[caption], caption)\n",
    "            for caption in captions_to_image_filepaths.keys()\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_ids, image_paths, caption = self.instances[idx]\n",
    "\n",
    "        # shape: (height, width, channels), dtype: uint8\n",
    "        images = [cv2.imread(image_path) for image_path in image_paths]\n",
    "        # cv2.imread loads images in BGR (blue, green, red) order\n",
    "        images = [cv2.cvtColor(image, cv2.COLOR_BGR2RGB) for image in images]\n",
    "\n",
    "        return {\"image_ids\": image_ids, \"images\": images, \"caption\": caption}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e0c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_pubmed_dataset = ArchPubmedCaptionsDataset(data_root='../datasets/ARCH', split=\"all\")\n",
    "\n",
    "print(len(arch_pubmed_dataset.instances))\n",
    "arch_pubmed_dataset.instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ec08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instance = arch_pubmed_dataset.__getitem__(0)\n",
    "\n",
    "print(test_instance['caption'], '\\n')\n",
    "print(\"Total images:\", len(test_instance['images']), '\\n')\n",
    "\n",
    "for i, img_id in enumerate(test_instance['image_ids']):\n",
    "    print(img_id)\n",
    "    img=test_instance['images'][i]\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724fa203",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instance = arch_pubmed_dataset.__getitem__(19)\n",
    "\n",
    "print(test_instance['caption'], '\\n')\n",
    "print(\"Total images:\", len(test_instance['images']), '\\n')\n",
    "\n",
    "for i, img_id in enumerate(test_instance['image_ids']):\n",
    "    print(img_id)\n",
    "    img=test_instance['images'][i]\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ced4eb",
   "metadata": {},
   "source": [
    "Check that all the images recorded in the instances exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ba192",
   "metadata": {},
   "outputs": [],
   "source": [
    "exist_status_list = [[os.path.exists(img_path) for img_path in img_paths] for img_ids, img_paths, img_caption in arch_pubmed_dataset.instances]\n",
    "\n",
    "# 3309 unique uuids, 3309 images, 3309 entries in the captions path\n",
    "sum([sum(sublist) for sublist in exist_status_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19299822",
   "metadata": {},
   "outputs": [],
   "source": [
    "exist_status_list_compressed = [all(sublist) for sublist in exist_status_list]\n",
    "# All images exit where they should, Same as the number of unique captions\n",
    "all(exist_status_list_compressed), sum(exist_status_list_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e3379",
   "metadata": {},
   "source": [
    "## Books Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8cd707",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_set_dir = '../datasets/ARCH/books_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc258e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(books_set_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68d5a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(books_set_dir +'/images')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a9301",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Old Version\n",
    "\n",
    "# class ArchBooksCaptionsDataset(Dataset):\n",
    "#     r\"\"\"\n",
    "#     A PyTorch dataset to read ARCH Books dataset and provide it completely\n",
    "#     unprocessed. This dataset is used by various task-specific datasets\n",
    "#     in :mod:`~virtex.data.datasets` module.\n",
    "\n",
    "#     Args:\n",
    "#         data_root: Path to the ARCH dataset root directory.\n",
    "#         split: Name of ARCH split to read. One of ``{\"train\", \"val\"}``.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, data_root: str, split: str=''):\n",
    "\n",
    "# #         TODO: change after splitting the caption files into train and validation\n",
    "# #         \n",
    "# #         # Get paths to image directory and annotation file.\n",
    "# #         image_dir = os.path.join(data_root, \"pubmed_set/images\", f\"{split}\")\n",
    "# #         captions = json.load(\n",
    "# #             open(os.path.join(data_root, \"pubmed_set\", f\"captions_{split}.json\"))\n",
    "# #         )\n",
    "        \n",
    "#         # Get paths to image directory and annotation file.\n",
    "#         image_dir = os.path.join(data_root, \"books_set/images\")\n",
    "#         captions = json.load(\n",
    "#             open(os.path.join(data_root, \"books_set\", \"captions.json\"))\n",
    "#         )\n",
    "                \n",
    "#         # Collect list of captions for each figure.\n",
    "#         captions_per_figure: Dict[int, List[str]] = defaultdict(list)\n",
    "#         for idx, ann in captions.items():\n",
    "#             captions_per_figure[ann['figure_id']].append(ann['caption'])\n",
    "#         #print(captions_per_image)\n",
    "        \n",
    "#         # Collect image file for each image (by its ID).\n",
    "#         image_filepaths: Dict[int, str] = {\n",
    "#             ann[\"uuid\"]: os.path.join(image_dir, f\"{ann['uuid']}.png\")\n",
    "#             for idx, ann in captions.items()\n",
    "#         }\n",
    "            \n",
    "#         # Collect list of images and image paths for each figure.\n",
    "#         images_per_figure: Dict[int, List[str]] = defaultdict(list)\n",
    "#         image_filepaths_per_figure: Dict[int, List[str]] = defaultdict(list)\n",
    "#         for idx, ann in captions.items():\n",
    "#             images_per_figure[ann['figure_id']].append(ann['uuid'])\n",
    "#             image_filepaths_per_figure[ann['figure_id']].append(image_filepaths[ann[\"uuid\"]])\n",
    "#         #print(captions_per_image)\n",
    "            \n",
    "        \n",
    "#         # Keep all annotations in memory. Make a list of tuples, each tuple\n",
    "#         # is ``(figure_id, list[img_ids], list[img_file_paths], list[captions])``.\n",
    "#         self.instances = [\n",
    "#             (figure_id, images_per_figure[figure_id],\n",
    "#              image_filepaths_per_figure[figure_id], captions_per_figure[figure_id])\n",
    "#             for figure_id in captions_per_figure.keys()\n",
    "#         ]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.instances)\n",
    "\n",
    "#     def __getitem__(self, idx: int):\n",
    "#         figure_id, image_ids, image_paths, captions = self.instances[idx]\n",
    "        \n",
    "#         images = []\n",
    "#         for image_path in image_paths:\n",
    "#             # shape: (height, width, channels), dtype: uint8\n",
    "#             image = cv2.imread(image_path)\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#             images.append(image)\n",
    "        \n",
    "#         return {\"figure_id\": figure_id, \"image_ids\": image_ids, \"images\": images, \"captions\": captions}\n",
    "\n",
    "\n",
    "## test\n",
    "\n",
    "# arch_books_dataset = ArchBooksCaptionsDataset('../datasets/ARCH')\n",
    "# arch_books_dataset.instances\n",
    "\n",
    "# test_instance = arch_books_dataset.__getitem__(9)\n",
    "# print(test_instance.keys())\n",
    "\n",
    "# print('figure_id:', test_instance['figure_id'])\n",
    "# print('image_ids:', test_instance['image_ids'])\n",
    "\n",
    "# for image in test_instance['images']:\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()\n",
    "    \n",
    "# print('captions:\\n', '-'*80, '\\n', '\\n\\n'.join(test_instance['captions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d887b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchBooksCaptionsDataset(Dataset):\n",
    "    r\"\"\"\n",
    "    A PyTorch dataset to read ARCH Books dataset and provide it completely\n",
    "    unprocessed. This dataset is used by various task-specific datasets\n",
    "    in :mod:`~virtex.data.datasets` module.\n",
    "\n",
    "    Args:\n",
    "        data_root: Path to the ARCH dataset root directory.\n",
    "        split: Name of ARCH split to read. One of ``{\"train\", \"val\", \"all\"}``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_root: str, split: str=''):\n",
    "\n",
    "        # Get path to image directory and record the extensions\n",
    "        image_dir = os.path.join(data_root, \"books_set/images\")\n",
    "        uuids_to_extensions = {\n",
    "            file_name.split('.')[0]: file_name.split('.')[1]\n",
    "            for file_name in os.listdir(image_dir)\n",
    "        }\n",
    "        #print(uuids_to_extensions)\n",
    "        \n",
    "        # Get path to the annotation file\n",
    "        captions = json.load(\n",
    "            open(os.path.join(data_root, \"annotations\", f\"captions_{split}.json\"))\n",
    "        )\n",
    "        #print(captions)\n",
    "        \n",
    "        # Collect list of uuids and file paths for each caption\n",
    "        captions_to_uuids: Dict[str, List[str]] = defaultdict(list)\n",
    "        captions_to_image_filepaths: Dict[str, List[str]] = defaultdict(list)\n",
    "        for idx, ann in captions.items():\n",
    "            if ann['uuid'] in uuids_to_extensions.keys():\n",
    "                \n",
    "                # uuids_to_extensions contains only image uuids from the image dir\n",
    "                # this means that only uuids with exisitng images will be added\n",
    "                captions_to_uuids[ann['caption']].append(ann['uuid'])\n",
    "                captions_to_image_filepaths[ann['caption']].append(ann['path'])\n",
    "        #print(captions_to_uuids)\n",
    "\n",
    "        # Keep all annotations in memory. Make a list of tuples, each tuple\n",
    "        # is ``(list[image_id], list[file_path], captions)``.\n",
    "        self.instances = [\n",
    "            (captions_to_uuids[caption], captions_to_image_filepaths[caption], caption)\n",
    "            for caption in captions_to_image_filepaths.keys()\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_ids, image_paths, caption = self.instances[idx]\n",
    "\n",
    "        # shape: (height, width, channels), dtype: uint8\n",
    "        images = [cv2.imread(image_path) for image_path in image_paths]\n",
    "        # cv2.imread loads images in BGR (blue, green, red) order\n",
    "        images = [cv2.cvtColor(image, cv2.COLOR_BGR2RGB) for image in images]\n",
    "\n",
    "        return {\"image_ids\": image_ids, \"images\": images, \"caption\": caption}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_books_dataset = ArchBooksCaptionsDataset(data_root='../datasets/ARCH', split=\"all\")\n",
    "print(len(arch_books_dataset.instances))\n",
    "arch_books_dataset.instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030da4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instance = arch_books_dataset.__getitem__(0)\n",
    "\n",
    "print(test_instance['caption'], '\\n')\n",
    "print(\"Total images:\", len(test_instance['images']), '\\n')\n",
    "\n",
    "for i, img_id in enumerate(test_instance['image_ids']):\n",
    "    print(img_id)\n",
    "    img=test_instance['images'][i]\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c057727",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instance = arch_books_dataset.__getitem__(5)\n",
    "\n",
    "print(test_instance['caption'], '\\n')\n",
    "print(\"Total images:\", len(test_instance['images']), '\\n')\n",
    "\n",
    "for i, img_id in enumerate(test_instance['image_ids']):\n",
    "    print(img_id)\n",
    "    img=test_instance['images'][i]\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instance = arch_books_dataset.__getitem__(18)\n",
    "\n",
    "print(test_instance['caption'], '\\n')\n",
    "print(\"Total images:\", len(test_instance['images']), '\\n')\n",
    "\n",
    "for i, img_id in enumerate(test_instance['image_ids']):\n",
    "    print(img_id)\n",
    "    img=test_instance['images'][i]\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b088b7",
   "metadata": {},
   "source": [
    "## Unified Dataset Class for ARCH Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2835b153",
   "metadata": {},
   "source": [
    "Once happy, I moved the dataset class to `virtex/data/datasets/arch_captions.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from virtex.data.datasets.arch_captions import ArchCaptionsDatasetRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786a2eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ArchCaptionsDatasetRaw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc2408a",
   "metadata": {},
   "source": [
    "### Books Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_books_dataset = ArchCaptionsDatasetRaw(data_root='../datasets/ARCH',\n",
    "                                            split=\"all\",\n",
    "                                            source=\"books\")\n",
    "len(arch_books_dataset.instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instance = arch_books_dataset.__getitem__(0)\n",
    "\n",
    "print(test_instance['caption'], '\\n')\n",
    "print(\"Total images:\", len(test_instance['images']), '\\n')\n",
    "\n",
    "for i, img_id in enumerate(test_instance['image_ids']):\n",
    "    print(img_id)\n",
    "    img=test_instance['images'][i]\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92cc15a",
   "metadata": {},
   "source": [
    "### PubMed Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d309fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_pubmed_dataset = ArchCaptionsDatasetRaw(data_root='../datasets/ARCH',\n",
    "                                             split=\"all\",\n",
    "                                             source=\"pubmed\")\n",
    "len(arch_pubmed_dataset.instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b0a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instance = arch_pubmed_dataset.__getitem__(19)\n",
    "\n",
    "print(test_instance['caption'], '\\n')\n",
    "print(\"Total images:\", len(test_instance['images']), '\\n')\n",
    "\n",
    "for i, img_id in enumerate(test_instance['image_ids']):\n",
    "    print(img_id)\n",
    "    img=test_instance['images'][i]\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05243784",
   "metadata": {},
   "source": [
    "### Both Sets Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be0455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_dataset = ArchCaptionsDatasetRaw(data_root='../datasets/ARCH',\n",
    "                                      split=\"all\",\n",
    "                                      source=\"both\")\n",
    "len(arch_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instance = arch_dataset.__getitem__(0) # same as the 0th example in the books dataset\n",
    "\n",
    "print(test_instance['caption'], '\\n')\n",
    "print(\"Total images:\", len(test_instance['images']), '\\n')\n",
    "\n",
    "for i, img_id in enumerate(test_instance['image_ids']):\n",
    "    print(img_id)\n",
    "    img=test_instance['images'][i]\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009caf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as the 19th example in the pubmed dataset \n",
    "# there are 3210 examples in the books set\n",
    "# pubmed set is concatenated to it -> 3239 gives the 19th example in the pubmed set\n",
    "test_instance = arch_dataset.__getitem__(3229)\n",
    "# test_instance = arch_dataset.__getitem__(1298)\n",
    "\n",
    "print(test_instance['caption'], '\\n')\n",
    "print(\"Total images:\", len(test_instance['images']), '\\n')\n",
    "\n",
    "for i, img_id in enumerate(test_instance['image_ids']):\n",
    "    print(img_id)\n",
    "    img=test_instance['images'][i]\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:virtex] *",
   "language": "python",
   "name": "conda-env-virtex-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
