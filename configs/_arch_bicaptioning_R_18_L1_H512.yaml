# -----------------------------------------------------------------------------
# Base config: VirTex pretraining for our "base" bicaptioning model:
# ResNet-18 + (L = 1, H = 512) transformer trained for 500K iterations.
# -----------------------------------------------------------------------------
RANDOM_SEED: 0
AMP: true
CUDNN_BENCHMARK: true
CUDNN_DETERMINISTIC: false

DATA:
  ROOT: "datasets/ARCH" # changed: coco -> ARCH
  TOKENIZER_MODEL: "datasets/vocab/arch_10k.model" # changed: coco -> arch
  VOCAB_SIZE: 10000
  UNK_INDEX: 0
  SOS_INDEX: 1
  EOS_INDEX: 2
  MASK_INDEX: 3

  IMAGE_CROP_SIZE: 224
  MAX_CAPTION_LENGTH: 30 # TODO: check if it's enough

  IMAGE_TRANSFORM_TRAIN:
    - "random_resized_crop"
    - "color_jitter"
    - "normalize"
    - "tensor_horizontal_flip" # changed: flip the whole bag (tensor)

  IMAGE_TRANSFORM_VAL:
    - "smallest_resize"
    - "center_crop"
    - "normalize"

MODEL:
  NAME: "virtex"

  VISUAL:
    NAME: "torchvision::resnet18" # changed: 50 -> 18
    PRETRAINED: false
    FROZEN: false

  TEXTUAL:
    NAME: "transdec_postnorm::L1_H512_A8_F2048" # changed: H=512 (ARCH); A:=H/64=8, and F:=4H=2048 (VirTex)
    DROPOUT: 0.1

  DECODER:
    NAME: "beam_search"
    BEAM_SIZE: 5

OPTIM: # TODO: change ("sgd", 0.9, 0.0001) -> ("Adam", 1e-3)
  OPTIMIZER_NAME: "sgd"
  SGD_MOMENTUM: 0.9
  WEIGHT_DECAY: 0.0001

  LOOKAHEAD:
    USE: true
    ALPHA: 0.5
    STEPS: 5

  BATCH_SIZE: 256 # TODO: check what fits, originally 32
  CNN_LR: 0.2 # TODO: change ("sgd", 0.9, 0.0001) -> ("Adam", 1e-3)
  LR: 0.001
  NUM_ITERATIONS: 500000

  WARMUP_STEPS: 10000
  LR_DECAY_NAME: "cosine"

  NO_DECAY: ".*textual.(embedding|transformer).*(norm.*|bias)"
  CLIP_GRAD_NORM: 10.0
